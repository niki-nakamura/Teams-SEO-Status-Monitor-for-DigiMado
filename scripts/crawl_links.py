#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
from collections import deque

# èª¿æŸ»å¯¾è±¡ã®URLã‚’ digi-mado.jp ã«å¤‰æ›´
START_URL = "https://digi-mado.jp/"
BASE_DOMAIN = "digi-mado.jp"  # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ã«ä½¿ç”¨
TEAMS_WEBHOOK_URL = os.environ.get("TEAMS_WEBHOOK_URL")

visited = set()
# broken_links ã¯ (å‚ç…§å…ƒ, å£Šã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯, ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹) ã®ã‚¿ãƒ—ãƒ«å½¢å¼
broken_links = []

# ãƒ–ãƒ©ã‚¦ã‚¶é¢¨ã® User-Agent ã‚’è¨­å®š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
}

def is_internal_link(url):
    parsed = urlparse(url)
    return (parsed.netloc == "" or parsed.netloc.endswith(BASE_DOMAIN))

def crawl(start_url):
    queue = deque([start_url])
    while queue:
        current = queue.popleft()
        if current in visited:
            continue
        visited.add(current)

        if is_internal_link(current):
            try:
                resp = requests.get(current, headers=HEADERS, timeout=10)
                # ã‚‚ã—ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§ 403 ãŒè¿”ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ç™»éŒ²ã›ãšã«å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹
                if resp.status_code >= 400:
                    if current == START_URL and resp.status_code == 403:
                        print(f"[Info] ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ {current} ãŒ 403 ã‚’è¿”ã—ã¾ã—ãŸãŒã€ã‚¨ãƒ©ãƒ¼å¯¾è±¡ã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")
                    else:
                        broken_links.append((current, current, resp.status_code))
                        continue
                soup = BeautifulSoup(resp.text, 'html.parser')
                for a in soup.find_all('a', href=True):
                    link = urljoin(current, a['href'])
                    link = urlparse(link)._replace(fragment="").geturl()
                    if not is_internal_link(link):
                        check_status(link, current)
                    if link not in visited:
                        queue.append(link)
            except Exception as e:
                broken_links.append((current, current, f"Error: {str(e)}"))
        else:
            check_status(current, None)

def check_status(url, source):
    try:
        r = requests.head(url, headers=HEADERS, timeout=5)
        if r.status_code >= 400:
            ref = source if source else url
            broken_links.append((ref, url, r.status_code))
    except Exception as e:
        ref = source if source else url
        broken_links.append((ref, url, f"Error: {str(e)}"))

def send_teams_notification(broken):
    if not TEAMS_WEBHOOK_URL:
        print("TEAMS_WEBHOOK_URL is not set.")
        return

    msg = "\n"
    msg += "404ãƒã‚§ãƒƒã‚¯çµæœğŸ—£ğŸ“¢\n\n"
    msg += "ğŸ‘‡ä»¥ä¸‹ã®æ¤œå‡ºã•ã‚ŒãŸ404ï¼ˆã¾ãŸã¯ãƒªãƒ³ã‚¯åˆ‡ã‚Œï¼‰ã®æƒ…å ±ã§ã™ğŸ‘‡\n\n"

    if not broken:
        msg += "No broken links found!\n"
    else:
        for source, url, status in broken:
            msg += f"{url} [Status: {status}]\n"
            msg += f"æ¤œå‡ºè¨˜äº‹å…ƒï¼š{source}\n\n"

    try:
        r = requests.post(TEAMS_WEBHOOK_URL, json={"text": msg}, headers=HEADERS, timeout=10)
        # Teams ã¯æˆåŠŸæ™‚ã« 200 ã¾ãŸã¯ 204 ã‚’è¿”ã™ã®ã§ã€ãã‚Œä»¥å¤–ã¯ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†
        if r.status_code not in [200, 204]:
            print(f"Teams notification failed with status {r.status_code}: {r.text}")
    except Exception as e:
        print(f"Teams notification failed: {e}")

def main():
    print(f"Starting crawl from {START_URL}")
    crawl(START_URL)
    print("Crawl finished.")
    print(f"Detected {len(broken_links)} broken links.")
    send_teams_notification(broken_links)

if __name__ == "__main__":
    main()
